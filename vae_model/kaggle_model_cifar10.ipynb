{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ad400a-8fb9-4707-a437-00b9a5700cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.image as img\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "random_seed = 42\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d2da8e6-8a8a-4c05-aa17-cfcc2060b508",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "X = []       \n",
    "labels = [] # this is not used in the training\n",
    "for path in ['/kaggle/input/cifar10/cifar10/train/*', '/kaggle/input/cifar10/cifar10/test/*']:\n",
    "    for (i, dirname) in enumerate(glob.glob(path)):\n",
    "        for filename in tqdm(glob.glob(os.path.join(dirname, '*'))):\n",
    "            file = img.imread(filename)\n",
    "            X.append(file)\n",
    "            labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81aedf15-3365-478a-b1db-f99ab6da1920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 4 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(X)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m11451\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 4 were indexed"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "\n",
    "print(X.shape)\n",
    "plt.imshow(X[11451,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315b2e0-8ae6-4f07-b7f0-03487dc67547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "X = torch.tensor(X)\n",
    "X = X.permute(0, 3,1,2)\n",
    "image_loader = DataLoader(X, batch_size = batch_size, shuffle = True)\n",
    "# We only care about reconstruction, so we don't need to compute accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32ff80a-899e-4973-8eb2-46d13fa28931",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d572c538-70ba-4b31-a3c1-069b4a03b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, *args):\n",
    "        super(Reshape, self).__init__()\n",
    "        self.shape = args\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x.view(self.shape)\n",
    "    \n",
    "class Trim(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Trim, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x[:, :, :32, :32]\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # [(Wâˆ’K+2P)/S]+1\n",
    "        super(VAE, self).__init__()\n",
    "        # Unfortunately, we cannot have maxunpool2d in nn.Sequential, so we have to write them out\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, stride = 2, padding = 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, stride = 2, padding = 1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, stride = 2, padding = 1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        self.linear_mean = nn.Linear(2048, 100)\n",
    "        self.linear_logvar = nn.Linear(2048, 100)\n",
    "\n",
    "        self.linear = nn.Linear(100, 2048)\n",
    "        self.reshape = Reshape(-1, 128, 4, 4)\n",
    "        self.deconv1 = nn.ConvTranspose2d(128, 64, 3, stride = 2, padding=0)\n",
    "        self.deconv2 = nn.ConvTranspose2d(64, 32, 3, stride = 2, padding=1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(32, 3, 3, stride = 2, padding=1)\n",
    "        self.trim = Trim()\n",
    "    \n",
    "    def reparameterized(self, mean, var):\n",
    "        eps = torch.randn(mean.size(0), mean.size(1)).to(device)\n",
    "        z = mean + eps * torch.exp(var / 2.)\n",
    "        return z\n",
    "\n",
    "    def encode(self, x): # Using silu instead of relu here\n",
    "        x = F.silu(self.conv1(x))\n",
    "        x = F.silu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.flatten(x)\n",
    "        mean = self.linear_mean(x)\n",
    "        var = self.linear_logvar(x)\n",
    "        z = self.reparameterized(mean, var)\n",
    "        return mean, var, z\n",
    "\n",
    "    def decode(self, z):\n",
    "        z = self.linear(z)\n",
    "        z = self.reshape(z)\n",
    "        z = F.silu(self.deconv1(z))\n",
    "        z = F.silu(self.deconv2(z))\n",
    "        z = F.silu(self.deconv3(z))\n",
    "        z = self.trim(z)\n",
    "        z = F.sigmoid(z)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean, var, z = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return mean, var, z\n",
    "\n",
    "model = VAE().to(device)\n",
    "model2 = VAE().to(device)\n",
    "# print(X.shape)\n",
    "\n",
    "x = torch.randn(1, 3, 32, 32).to(device)\n",
    "model(x)\n",
    "summary(model, (3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1deeb3a6-4edc-4cff-80b5-900c2d6edee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "KL_weight = 0.000075 # The KL divergence is much larger than MAE loss, scaling it down\n",
    "\n",
    "train_losses_total, train_losses_total_avg = [], []\n",
    "train_losses_reconstruction, train_losses_reconstruction_avg = [], []\n",
    "train_losses_KL, train_losses_KL_avg = [], []\n",
    "for epoch in range(epochs):\n",
    "    train_loss_total = 0.0\n",
    "    train_loss_reconstruction = 0.0\n",
    "    train_loss_KL = 0.0\n",
    "    for images in tqdm(image_loader):\n",
    "        images = images.to(device)\n",
    "        mean, var, outputs = model(images)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss1 = criterion(outputs, images)\n",
    "        loss2 = torch.mean(-0.5 * torch.sum(1 + var - mean**2 - torch.exp(var),axis=1),axis=0) # sum over latent dimension, not batch\n",
    "        # -0.5 * torch.sum(1 + var - mean**2 - torch.exp(var), axis = 1).mean()\n",
    "        # loss2 = torch.atan(loss2) / (np.pi / 2) # scaling it to [0,1]        \n",
    "        loss = loss1  + KL_weight * loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with torch.no_grad():\n",
    "            train_losses_reconstruction.append(loss1.item())\n",
    "            train_losses_KL.append(loss2.item())\n",
    "            train_losses_total.append(loss.item())\n",
    "            train_loss_reconstruction += loss1.item() * images.size(0)\n",
    "            train_loss_KL += loss2.item() * images.size(0)\n",
    "            train_loss_total += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss_total /= len(image_loader)\n",
    "    train_loss_reconstruction /= len(image_loader)\n",
    "    train_loss_KL /= len(image_loader)\n",
    "    \n",
    "    train_losses_total_avg.append(train_loss_total)\n",
    "    train_losses_reconstruction_avg.append(train_loss_reconstruction)\n",
    "    train_losses_KL_avg.append(train_loss_KL)\n",
    "    \n",
    "    print('-----------------------------------------------------')\n",
    "    print(f'Epoch{epoch + 1}')\n",
    "    print(f'Total loss = {train_loss_total:.3f}')\n",
    "    print(f'Reconstruction loss = {train_loss_reconstruction:.3f}, KL = {train_loss_KL:.3f}')\n",
    "\n",
    "    log = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict()\n",
    "    }\n",
    "\n",
    "    torch.save(log, f'model1_log_{epoch + 1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ba690-2204-4b4f-bcbd-e6463f2defa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize = (6, 15))\n",
    "\n",
    "train_losses_reconstruction_avg = [np.mean(train_losses_reconstruction[i:i+25]) for i in range(3750)]\n",
    "axs[0].plot(np.arange(epochs * 1875), train_losses_reconstruction, color = 'b', label = 'mini-batches')\n",
    "axs[0].plot(np.arange(0, epochs * 1875, 25), train_losses_reconstruction_avg, color = 'r', label = 'running average')\n",
    "axs[0].set_ylim(0,0.1)\n",
    "axs[0].set_title('Reconstruction losses')\n",
    "\n",
    "train_losses_KL_avg = [np.mean(train_losses_KL[i:i+25]) for i in range(3750)]\n",
    "axs[1].plot(np.arange(epochs * 1875), train_losses_KL, color = 'b', label = 'mini-batches')\n",
    "axs[1].plot(np.arange(0, epochs * 1875, 25), train_losses_KL_avg, color = 'r', label = 'running average')\n",
    "axs[1].set_title('KL losses')\n",
    "\n",
    "train_losses_total_avg = [np.mean(train_losses_total[i:i+20]) for i in range(3750)]\n",
    "axs[2].plot(np.arange(epochs * 1875), train_losses_total, color = 'b', label = 'mini-batches')\n",
    "axs[2].plot(np.arange(0, epochs * 1875, 25), train_losses_total_avg, color = 'r', label = 'running average')\n",
    "axs[2].set_ylim(0,0.1)\n",
    "axs[2].set_title('Scaled losses')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6f42d-139c-437b-92d7-0a0593a152f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5480f6a-1711-4018-ba50-fc3d3ef06d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 6, figsize = (20, 5))\n",
    "for i in range(6):\n",
    "    axs[0][i].imshow(X[i+100,:,:,:].permute(1,2,0))\n",
    "_, _, new_fig = model(X[100:106, :, :, :].to(device))\n",
    "for i in range(6):\n",
    "    axs[1][i].imshow(new_fig[i].permute(1,2,0).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ea23a7-ab95-4b7b-983e-e8fd2ca033ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c29532-7316-4371-b390-15d933af0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating 60 images using model\n",
    "\n",
    "fig, axs = plt.subplots(10,6, figsize = (50, 30))\n",
    "\n",
    "names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "sixty_images_latent = torch.randn(60, 100).to(device)\n",
    "sixty_images = model.decode(sixty_images_latent)\n",
    "print(sixty_images.shape)\n",
    "sixty_images = sixty_images.permute(0, 3, 2, 1).cpu().detach().numpy()\n",
    "\n",
    "for i in range(10):\n",
    "    for j in range(6):\n",
    "        idx = (6) * (i-1) + (j)\n",
    "        axs[i][j].imshow(sixty_images[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
